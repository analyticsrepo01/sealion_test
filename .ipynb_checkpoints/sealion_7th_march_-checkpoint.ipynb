{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Mistral and Mixtral 8x7B Models\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "Open in Vertex AI Workbench\n",
    "    </a> (A Python-3 CPU notebook is recommended)\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying prebuilt [Mistral](https://mistral.ai/) and Mixtral 8x7B models in Vertex AI.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Deploy prebuilt [Mistral models](https://huggingface.co/mistralai) with [vLLM](https://github.com/vllm-project/vllm) containers\n",
    "    - [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1): pretrained generative text model with 7 billion parameters\n",
    "    - [mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1): Instruction fine-tuned version of the Mistral-7B-v0.1 generative text model\n",
    "- Deploy prebuit [Mixtral 8x7B model](https://huggingface.co/mistralai) with [vLLM](https://github.com/vllm-project/vllm) containers\n",
    "    - [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1): pretrained Mixture of Experts (MoE) model with 8 branches\n",
    "    - [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1): Instruction fine-tuned version of the Mixture of Experts (MoE) model with 8 branches\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Cloud NL API pricing](https://cloud.google.com/natural-language/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "### Colab only\n",
    "Run the following commands for Colab and skip this section if you are using Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 33649,
     "status": "ok",
     "timestamp": 1709776780883,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "2707b02ef5df",
    "outputId": "3f8ee22a-a640-4f70-e73f-567f618f077d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()\n",
    "    # Install gdown for downloading example training images.\n",
    "    ! pip3 install gdown\n",
    "\n",
    "    # Restart the notebook kernel after installs.\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46d25fe73955"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c75c2c1fa6e0",
    "outputId": "22b37fde-bb3d-4389-ec8c-ad4534584f37",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.36.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (4.36.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/py311/lib/python3.11/site-packages (from transformers==4.36.0) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py311/lib/python3.11/site-packages (from requests->transformers==4.36.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py311/lib/python3.11/site-packages (from requests->transformers==4.36.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py311/lib/python3.11/site-packages (from requests->transformers==4.36.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py311/lib/python3.11/site-packages (from requests->transformers==4.36.0) (2023.11.17)\n",
      "Requirement already satisfied: accelerate==0.23.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/py311/lib/python3.11/site-packages (from accelerate==0.23.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from accelerate==0.23.0) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py311/lib/python3.11/site-packages (from accelerate==0.23.0) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/py311/lib/python3.11/site-packages (from accelerate==0.23.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from accelerate==0.23.0) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/envs/py311/lib/python3.11/site-packages (from accelerate==0.23.0) (0.21.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.23.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/py311/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.23.0) (12.4.99)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py311/lib/python3.11/site-packages (from huggingface-hub->accelerate==0.23.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/py311/lib/python3.11/site-packages (from huggingface-hub->accelerate==0.23.0) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py311/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate==0.23.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py311/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.23.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py311/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.23.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py311/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.23.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py311/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.23.0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/py311/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate==0.23.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -q transformers==4.36.0\n",
    "! pip3 install -q accelerate==0.23.0\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --quiet --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### Setup Google Cloud project\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API, Compute Engine API and Cloud Natural Language API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,language.googleapis.com).\n",
    "\n",
    "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
    "\n",
    "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "### Define environment variables\n",
    "\n",
    "Set the following variables for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the specified region (`REGION`). Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2582,
     "status": "ok",
     "timestamp": 1709776814807,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "855d6b96f291",
    "outputId": "77bd92a2-3011-496c-d541-bce23e9c683a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-project-0004-346516\n",
      "my-project-0004-346516\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Cloud project id.\n",
    "\n",
    "# GCP_PROJECT= PROJECT_ID=project_id= !(gcloud config get-value core/project)\n",
    "PROJECT_ID= !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "GCP_PROJECT = PROJECT_ID\n",
    "VERTEX_API_LOCATION = LOCATION = REGION = 'asia-southeast1'\n",
    "\n",
    "print(GCP_PROJECT)\n",
    "print(PROJECT_ID)\n",
    "\n",
    "BUCKET_NAME = 'my-project-0004-bucket' # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output with gs:// prefix.\n",
    "BUCKET_URI = \"gs://my-project-0004-bucket\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "import os\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"255766800726-compute@developer.gserviceaccount.com\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "### Initialize Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1709776814807,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "12cd25839741"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31e354635b4449bb7b688212439bead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34143664035046cb8b0baa43e42ac5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_SEA_BPE.py:   0%|          | 0.00/7.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/aisingapore/sea-lion-3b:\n",
      "- tokenization_SEA_BPE.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentencepiece'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# please use transformers 4.34.1\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maisingapore/sea-lion-3b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maisingapore/sea-lion-3b\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSea lion in the sea\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:770\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m tokenizer_auto_map[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 770\u001b[0m tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n",
      "File \u001b[0;32m/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:500\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m    488\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[1;32m    489\u001b[0m     repo_id,\n\u001b[1;32m    490\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    499\u001b[0m )\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:200\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mImport a module on the cache directory for modules and extract a class from it.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    `typing.Type`: The class looked for.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m module_path \u001b[38;5;241m=\u001b[39m module_path\u001b[38;5;241m.\u001b[39mreplace(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[0;32m/opt/conda/envs/py311/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/aisingapore/sea-lion-3b/4a4d91f10893d9973c43f064425b704de4f56083/tokenization_SEA_BPE.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copyfile\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspm\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m processors\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AddedToken, PreTrainedTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentencepiece'"
     ]
    }
   ],
   "source": [
    "# please use transformers 4.34.1\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aisingapore/sea-lion-3b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"aisingapore/sea-lion-3b\", trust_remote_code=True)\n",
    "\n",
    "tokens = tokenizer(\"Sea lion in the sea\", return_tensors=\"pt\")\n",
    "output = model.generate(tokens[\"input_ids\"], max_new_tokens=20, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1709780078377,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "# The pre-built serving docker images with vLLM\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240112_0916_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1709776814808,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 4096,\n",
    "    use_openai_server: bool = False,\n",
    "    use_chat_completions_if_openai_server: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys Mistral models with vLLM on Vertex AI.\n",
    "\n",
    "    Args:\n",
    "        model_name: Display name of the model.\n",
    "        model_id: Model ID or path to model weights.\n",
    "        service_account: Service account for model uploading and deployment.\n",
    "        machine_type: Deployment machine type.\n",
    "        accelerator_type: Deployment accelerator type.\n",
    "        accelerator_count: Number of accelerators to use.\n",
    "        max_model_len: Maximum model length.\n",
    "        use_openai_server: Whether to use the OpenAI-format vLLM model server.\n",
    "        use_chat_completions_if_openai_server: If the OpenAI model server is\n",
    "            used, whether to use the chat completion API as opposed to the text\n",
    "            completion API. The vLLM text completion API mimics the OpenAI text\n",
    "            completion API:\n",
    "            https://platform.openai.com/docs/api-reference/completions/create.\n",
    "            It has two required parameters: the model ID to direct requests to\n",
    "            and the prompt. The response includes a \"choices\" field that\n",
    "            contains the generated text and a \"usage\" field that contains token\n",
    "            counts. The vLLM chat completion API mimics the OpenAI chat\n",
    "            completion API:\n",
    "            https://platform.openai.com/docs/api-reference/chat/create. It has\n",
    "            two required parameters: the model ID to direct requests to and\n",
    "            \"messages\" which is a sequence of system/user/assistant/tool\n",
    "            messages that can represent a multi-turn chat conversation. The\n",
    "            response includes a \"choices\" field that contains the generated\n",
    "            message from a role and a \"usage\" field that contains token counts.\n",
    "\n",
    "    Returns:\n",
    "        Model instance and endpoint instance.\n",
    "    \"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    dtype = \"float16\" # \"bfloat16\"\n",
    "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
    "        dtype = \"float16\"\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        \"--disable-log-stats\",\n",
    "        \"--trust-remote-code\",\n",
    "    ]\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    }\n",
    "    if use_openai_server:\n",
    "        if use_chat_completions_if_openai_server:\n",
    "            serving_container_predict_route = \"/v1/chat/completions\"\n",
    "        else:\n",
    "            serving_container_predict_route = \"/v1/completions\"\n",
    "    else:\n",
    "        serving_container_predict_route = \"/generate\"\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\n",
    "            \"python\",\n",
    "            \"-m\",\n",
    "            (\n",
    "                \"vllm.entrypoints.api_server\"\n",
    "                if not use_openai_server\n",
    "                else \"vllm.entrypoints.openai.api_server\"\n",
    "            ),\n",
    "        ],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=serving_container_predict_route,\n",
    "        serving_container_health_route=\"/health\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKZ4CBJ2kYaW"
   },
   "source": [
    "## Deploy Prebuilt Mistral model with vLLM\n",
    "\n",
    "This section deploys the prebuilt Mistral model with [vLLM](https://github.com/vllm-project/vllm) on a Vertex endpoint. The model deployment step will take ~15 minutes to complete.\n",
    "\n",
    "vLLM is a highly optimized LLM serving framework which can significantly increase serving throughput. The higher QPS you have, the more benefits you get using vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25b5b3a44cf8"
   },
   "source": [
    "Set the prebuilt model id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1709779082178,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "10547af949fc"
   },
   "outputs": [],
   "source": [
    "prebuilt_model_id = 'aisingapore/sea-lion-7b-instruct' # \"aisingapore/sealion3b\"  #\n",
    "## @param [\"mistralai/Mistral-7B-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 992217,
     "status": "error",
     "timestamp": 1709780078376,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "03d504bcd60b",
    "outputId": "b1345811-c0af-4aa8-c01d-c7f9df791f8d"
   },
   "outputs": [],
   "source": [
    "# Find Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 1 L4 to deploy Mistral 7B.\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets 2 V100s to deploy Mistral 7B.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Sets 2 T4s to deploy Mistral 7B.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_T4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Sets 1 A100 (40G) to deploy Mistral 7B.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Larger setting of `max-model-len` can lead to higher requirements on\n",
    "# `gpu-memory-utilization` and GPU configuration.\n",
    "max_model_len = 4096\n",
    "\n",
    "def deploy_model_with_config(machine_type, accelerator_type, accelerator_count ):\n",
    "\n",
    "    model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"mistral-serve-vllm\"),\n",
    "    model_id=prebuilt_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    "    use_openai_server=False,\n",
    "    use_chat_completions_if_openai_server=False,\n",
    ")\n",
    "\n",
    "    return(model, endpoint)\n",
    "\n",
    "try:\n",
    "    # Code that might potentially cause an error\n",
    "    machine_type = \"g2-standard-8\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 1\n",
    "    model, endpoint = deploy_model_with_config(machine_type, accelerator_type, accelerator_count )\n",
    "except :  # Replace 'ErrorType' with the specific error you want to catch\n",
    "    try :\n",
    "        print(\"Error may becaused due to machine unavailable \")\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "        accelerator_count = 1\n",
    "        model, endpoint = deploy_model_with_config(machine_type, accelerator_type, accelerator_count )\n",
    "    except :\n",
    "        print(\"Error may becaused due to machine unavailable - A100s - trying T4s \")\n",
    "        machine_type = \"n1-standard-16\"\n",
    "        accelerator_type = \"NVIDIA_TESLA_T4\"\n",
    "        accelerator_count = 2\n",
    "        model, endpoint = deploy_model_with_config(machine_type, accelerator_type, accelerator_count )\n",
    "else:\n",
    "    # Code to execute if there's no error in the 'try' block\n",
    "    print(\"Error may becaused due to machine unavailable of any type \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRR11SWykYaX"
   },
   "source": [
    "NOTE: If you see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint, the model server is likely still initializing. Please retry later.\n",
    "\n",
    "NOTE: If you receive `InternalServerError: 500 System error` during the deployment, most likely the operation failed due to unavailability of resources. Either retry or use a different accelerator type.\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a7948c56e3d"
   },
   "source": [
    "### Run sample prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eB9O7Y3_6PG"
   },
   "source": [
    "## Deploy Prebuilt Mixtral 8x7B model with vLLM\n",
    "\n",
    "This section deploys the prebuilt Mixtral 8x7B model with [vLLM](https://github.com/vllm-project/vllm) on a Vertex endpoint. The model deployment step will take ~40 minutes to complete.\n",
    "\n",
    "vLLM is a highly optimized LLM serving framework which can significantly increase serving throughput. The higher QPS you have, the more benefits you get using vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGJ7hOzH_6PG"
   },
   "source": [
    "Set the prebuilt model id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1709778083936,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "L4K7lxUr_6PI"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the endpoint name of\n",
    "#   the endpoint `endpoint` created in the cell above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_without_peft.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"My favourite condiment is\",\n",
    "        \"n\": 1,\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "\n",
    "# Reference the following code for using the OpenAI vLLM server.\n",
    "# import json\n",
    "# response = endpoint.raw_predict(\n",
    "#     body=json.dumps({\n",
    "#         \"model\": prebuilt_model_id,\n",
    "#         \"prompt\": \"My favourite condiment is\",\n",
    "#         \"n\": 1,\n",
    "#         \"max_tokens\": 200,\n",
    "#         \"temperature\": 1.0,\n",
    "#         \"top_p\": 1.0,\n",
    "#         \"top_k\": 10,\n",
    "#     }),\n",
    "#     headers={\"Content-Type\": \"application/json\"},\n",
    "# )\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRaBADRM4JEn"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_mistral.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-py311-py311",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-env-py311-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
